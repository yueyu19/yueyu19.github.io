---
title: 'The law of cosines: From high-school math to advanced proof techniques in convex optimization'
date: 2021-10-01
permalink: /posts/2012/08/blog-post-1/
mathjax: true
tags:
  - convex optimization
  - first-order optimization methods
---
Do you remeber the law of cosines from your high-school math classes? Did you know that it is an incredibly useful technique in proving the convergence of almost all of the first-order convex optimization methods? Find out more in this post. 

<img src="/images/cosine.png" width="200" height="200" img align='right'>

Where it all started
======

You probably have seen the [low of cosines](https://en.wikipedia.org/wiki/Law_of_cosines) before, likely in high school. Given a triangle, the law of cosines gives an identity between the cosine of one of the three angles and the length of the three sides, as shown by the image on the right.

Although traditionally considered for triangles in two or three dimensional spaces, the law of cosines actually hold in general $n$-dimensional vector space as well. 
Let $x^1, x^2, x^3\in\mathbb{R}^n$ be three points in a $n$-dimentional vector space. By completing the squares one can easily verify the following general form of law of cosines:

$$ \langle x^1-x^2, x^3-x^1\rangle=\frac{1}{2}||x^2-x^3||-\frac{1}{2}||x^1-x^3||-\frac{1}{2}||x^1-x^2||. $$

Notice the resemblance between the above equation and the traditional form of the law of cosines: the length of each side of the triangle is now measured by the \(\ell_2\) norm, and the cosine is hidden in the inner product term.

And there you have it, a secret skeleton key to unlock many seemingly enigmatic proofs in convex optimization! 

But how come that a simple equation can be that useful? Well, it turns out the equation natually embeds many terms common in the convergence proof of convex optimization methods. For example, when minimizing a function over a set, the minimizer is characterized by a variational inequality, where an inner product appears. On the other hand, the convergence proof of many common first-order convex optimization methods relies on showing some "disatance function to optimum" is decreasing. Such distance function is most commonly defined by a quadratic function, and the convergence proof usually starts by showing the difference between two quadratic functions is negative. 

Do you see it now? Both the inner product and the difference between two quadratic functions natually show up in the law of cosines! By providing a handy trick in dealing with these terms, the law of cosines save many proof writers, such as myself, from a tedious process of completing many, many squares.


Extensions
======

Non-quadratic distance function
------

Hadamard manifolds
------

Let's see an example
======

Want some more examples?
======
