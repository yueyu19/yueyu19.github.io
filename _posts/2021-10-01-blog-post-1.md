---
title: 'The law of cosines: A skeleton key in the convergence proofs of optimization methods'
date: 2021-10-01
permalink: /posts/2012/08/blog-post-1/
mathjax: true
tags:
  - convex optimization
  - first-order optimization methods
---
Do you remeber the law of cosines from your high-school math classes? Did you know that it is an incredibly useful technique in proving the convergence of almost all of the first-order optimization methods? Find out more in this post. 

<img src="/images/cosine.png" width="200" height="200" img align='right'>

The law of cosine
======

You probably have seen the [low of cosines](https://en.wikipedia.org/wiki/Law_of_cosines) before, likely in high school. Given a triangle, the law of cosines gives an identity between the cosine of one of the three angles and the length of the three sides, as shown by the image on the right.

Although traditionally considered for two or three dimensional triangles, the law of cosines actually hold in higher dimensions as well. 
Let $x^1, x^2, x^3\in\mathbb{R}^n$ be three points in a $n$-dimentional vector space. By completing the squares one can verify the following general form of law of cosines:

$$ \langle x^1-x^2, x^3-x^1\rangle=\frac{1}{2}||x^3-x^2||-\frac{1}{2}||x^3-x^1||-\frac{1}{2}||x^1-x^2||. $$

Notice the resemblance between the above equation and the traditional form of the law of cosines: the length of each side of the triangle is now measured by the $\ell_2$ norm, and the cosine is hidden in the inner product term.

And there you have it, a **skeleton key** to unlock many seemingly enigmatic proofs in convex optimization! 

But how can a simple equation be that useful? Well, it turns out the equation above natually embeds many terms common in the convergence proof of convex optimization methods. For example, when minimizing a function over a set, the minimizer is characterized by a variational inequality, where an inner product appears. On the other hand, the convergence proof of many common first-order convex optimization methods relies on showing some **disatance** to the optimum is decreasing. Such distance is commonly measured by a quadratic function, and the convergence proof usually starts by showing the difference between two quadratic functions is negative. 

Do you see it now? Both the inner product and the difference between two quadratic functions natually show up in the law of cosines! By providing a handy trick in dealing with these terms, the law of cosines save many proof writers, such as myself, from a tedious process of completing many, many squares.


Extensions
======

Over the years, many extensions of the law of cosines have been developed in the optimization literature. Here I will discuss two of the most popular ones. 

Non-quadratic distance function
------

A key ingrident in the law of cosines is the quadratic function. So what makes quadratic functions so special in the law of cosines? If we use some other functions instead, can we still get results similar to the law of cosines?

The answer is yes! All we need to do is to replace the quadratic function with something more general, known as the <em>Bregman divergence<em>.  

To this end, we consider a nonlinear differentiable function $f:\mathbb{R}^n\to\mathbb{R}$. Given two points $x^1, x^2\in\mathbb{R}^n$, the Bregman divergence between $x^1$ and $x^2$ associated with function $f$ is given as follows:

$$ B_f(x^1, x^2)=f(x^1)-f(x^2)-\langle \nabla f(x_2), x^1-x^2\rangle.$$  
  
In addition, using the above definition, you can verify that the following equation holds:
  
$$ \langle \nabla f(x^1)-\nabla f(x^2), x^3-x^1\rangle=B_f(x^3, x^2)-B_f(x^3, x^1||- B-f(x^1, x^2). $$  
  
You can chekc that if $f=\frac{1}{2}||x||^2$, then $B_f(x^1, x^2)=\frac{1}{2}||x^1-x^2||$, and the above equation becoems to (1). 
  
If function $f$ is convex, then $B_f(x^1, x^2)$ is always nonnegative. In this case, the Bregman divergence becomes a candidate function for measuring distance to optimum in optimization, as a generalization to quadratic functions. In fact, this generalization is the key difference between [gradient descent method](https://en.wikipedia.org/wiki/Gradient_descent) and [mirror descent method](https://www.sciencedirect.com/science/article/abs/pii/S0167637702002316), two of the most famous first-order optimization methods.   

Hadamard manifolds
------

Want to see how it is used in a convergence proof?
======

Want some more examples?
======
