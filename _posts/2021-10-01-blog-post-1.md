---
title: 'The law of cosines: From high-school math to advanced proof techniques in convex optimization'
date: 2021-10-01
permalink: /posts/2012/08/blog-post-1/
mathjax: true
tags:
  - convex optimization
  - first-order optimization methods
---
Do you remeber the law of cosines from your high-school math classes? Did you know that it is an incredibly useful technique in proving the convergence of almost all of the first-order convex optimization methods? Find out more in this post. 

<img src="/images/cosine.png" width="200" height="200" img align='right'>

Where it all started
======

You probably have seen the [low of cosines](https://en.wikipedia.org/wiki/Law_of_cosines) before, likely in high school. Given a triangle, the law of cosines gives an identity between the cosine of one of the three angles and the length of the three sides, as shown by the image on the right.

In N-dimensional simplex noise, the squared kernel summation radius $r^2$ is $\frac 1 2$
for all values of N. This is because the edge length of the N-simplex $s = \sqrt {\frac {N} {N + 1}}$
divides out of the N-simplex height $h = s \sqrt {\frac {N + 1} {2N}}$.
The kerel summation radius $r$ is equal to the N-simplex height $h$.

$$ r = h = \sqrt{\frac {1} {2}} = \sqrt{\frac {N} {N+1}} \sqrt{\frac {N+1} {2N}} $$

Extensions
======

Non-quadratic distance function
------

Hadamard manifolds
------

Let's see an example
======

Want some more examples?
======
